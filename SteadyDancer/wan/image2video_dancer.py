# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
import gc
import logging
import math
import os
import random
import sys
import types
from contextlib import contextmanager
from functools import partial
from typing import List, Optional, Tuple, Dict
import numpy as np
import torch
import torch.cuda.amp as amp
import torch.distributed as dist
import torchvision.transforms.functional as TF
from tqdm import tqdm

from .distributed.fsdp import shard_model
from .modules.clip import CLIPModel
from .modules.model_dancer import WanModel
from .modules.t5 import T5EncoderModel
from .modules.vae import WanVAE
from .utils.fm_solvers import (
    FlowDPMSolverMultistepScheduler,
    get_sampling_sigmas,
    retrieve_timesteps,
)
from .utils.fm_solvers_unipc import FlowUniPCMultistepScheduler


class WanI2VDancer:

    def __init__(
        self,
        config,
        checkpoint_dir,
        device_id=0,
        rank=0,
        t5_fsdp=False,
        dit_fsdp=False,
        use_usp=False,
        t5_cpu=False,
        init_on_cpu=True,
        st_cond_cfg=0.1,
        end_cond_cfg=0.5,
        wrapper=True,
    ):
        r"""
        Initializes the image-to-video generation model components.

        Args:
            config (EasyDict):
                Object containing model parameters initialized from config.py
            checkpoint_dir (`str`):
                Path to directory containing model checkpoints
            device_id (`int`,  *optional*, defaults to 0):
                Id of target GPU device
            rank (`int`,  *optional*, defaults to 0):
                Process rank for distributed training
            t5_fsdp (`bool`, *optional*, defaults to False):
                Enable FSDP sharding for T5 model
            dit_fsdp (`bool`, *optional*, defaults to False):
                Enable FSDP sharding for DiT model
            use_usp (`bool`, *optional*, defaults to False):
                Enable distribution strategy of USP.
            t5_cpu (`bool`, *optional*, defaults to False):
                Whether to place T5 model on CPU. Only works without t5_fsdp.
            init_on_cpu (`bool`, *optional*, defaults to True):
                Enable initializing Transformer Model on CPU. Only works without FSDP or USP.
        """
        self.device = torch.device(f"cuda:{device_id}")
        self.config = config
        self.rank = rank
        self.use_usp = use_usp
        self.t5_cpu = t5_cpu
        self.wrapper = wrapper
        self.num_train_timesteps = config.num_train_timesteps
        self.param_dtype = config.param_dtype

        shard_fn = partial(shard_model, device_id=device_id)
        if not  self.wrapper:
            self.text_encoder = T5EncoderModel(
                text_len=config.text_len,
                dtype=config.t5_dtype,
                device=torch.device('cpu'),
                checkpoint_path=os.path.join(checkpoint_dir, config.t5_checkpoint),
                tokenizer_path=os.path.join(checkpoint_dir, config.t5_tokenizer),
                shard_fn=shard_fn if t5_fsdp else None,
            )
            
        self.vae_stride = config.vae_stride
        self.patch_size = config.patch_size
        if not  self.wrapper:
            self.vae = WanVAE(
                vae_pth=os.path.join(checkpoint_dir, config.vae_checkpoint),
                device=self.device)

            self.clip = CLIPModel(
                dtype=config.clip_dtype,
                device=self.device,
                checkpoint_path=os.path.join(checkpoint_dir,
                                            config.clip_checkpoint),
                tokenizer_path=os.path.join(checkpoint_dir, config.clip_tokenizer))

        logging.info(f"Creating WanModel from {checkpoint_dir}")
        self.model = WanModel.from_pretrained(checkpoint_dir)
        self.model.eval().requires_grad_(False)
        self.model.to(torch.bfloat16)

        if t5_fsdp or dit_fsdp or use_usp:
            init_on_cpu = False

        if use_usp:
            from xfuser.core.distributed import get_sequence_parallel_world_size

            from .distributed.xdit_context_parallel_dancer import (
                usp_attn_forward,
                usp_dit_forward,
            )
            for block in self.model.blocks:
                block.self_attn.forward = types.MethodType(
                    usp_attn_forward, block.self_attn)
            self.model.forward = types.MethodType(usp_dit_forward, self.model)
            self.sp_size = get_sequence_parallel_world_size()
        else:
            self.sp_size = 1

        if dist.is_initialized():
            dist.barrier()
        if dit_fsdp:
            self.model = shard_fn(self.model)
        else:
            if not init_on_cpu:
                self.model.to(self.device)

        self.sample_neg_prompt = config.sample_neg_prompt
        self.st_cond_cfg, self.end_cond_cfg = st_cond_cfg, end_cond_cfg

    def load_lora_weights_(self, lora_path: str, adapter_name: str = "default", weight_name: str = None):
        """
        为WAN非diffusers版本的DiT模型加载LoRA权重
        
        Args:
            lora_path (str): LoRA权重路径
            adapter_name (str): 适配器名称
            weight_name (str, optional): 权重文件名
        """
        try:
            # 直接为transformer/DiT模型加载LoRA权重
            if hasattr(self, 'model') and self.model is not None:
                # 使用自定义的LoRA加载方法
                state_dict = self._load_lora_weights_to_dit(
                    self.model, 
                    lora_path, 
                    weight_name=weight_name,
                    adapter_name=adapter_name
                )
                print(f"Successfully loaded LoRA weights to DiT model from {lora_path}")
                return state_dict
            else:
                raise ValueError("No model (DiT) model found in the pipeline")
                
        except Exception as e:
            print(f"Error loading LoRA weights to DiT: {e}")
            

    def _load_lora_weights_to_dit(self, dit_model, lora_path: str, weight_name: str = None, adapter_name: str = "default"):
        """
        专门为DiT模型加载LoRA权重的内部方法
        
        Args:
            dit_model: DiT模型实例
            lora_path: LoRA权重路径
            weight_name: 权重文件名
            adapter_name: 适配器名称
        """
        import os
        import json
        from safetensors.torch import load_file
        
        # 处理不同的路径情况
        if os.path.isfile(lora_path):
            # 如果是文件路径
            if lora_path.endswith('.safetensors'):
                state_dict = load_file(lora_path)
            else:
                state_dict = torch.load(lora_path, map_location='cpu')
        elif os.path.isdir(lora_path):
            # 如果是目录路径，查找权重文件
            weight_files = []
            for file in os.listdir(lora_path):
                if file.endswith(('.pt', '.pth', '.bin', '.safetensors')):
                    weight_files.append(file)
            
            # 根据weight_name选择文件或使用默认规则
            target_file = None
            if weight_name:
                for file in weight_files:
                    if weight_name in file:
                        target_file = file
                        break
            else:
                # 优先选择包含'lora'的文件
                for file in weight_files:
                    if 'lora' in file.lower():
                        target_file = file
                        break
                # 如果没有找到，选择第一个文件
                if not target_file and weight_files:
                    target_file = weight_files[0]
            
            if target_file:
                full_path = os.path.join(lora_path, target_file)
                if full_path.endswith('.safetensors'):
                    state_dict = load_file(full_path)
                else:
                    state_dict = torch.load(full_path, map_location='cpu')
            else:
                raise FileNotFoundError(f"No valid weight files found in {lora_path}")
        else:
            # 假设是HuggingFace模型标识符
            try:
                from huggingface_hub import hf_hub_download
                # 下载权重文件
                if weight_name:
                    file_path = hf_hub_download(repo_id=lora_path, filename=weight_name)
                else:
                    # 尝试下载常见的LoRA权重文件名
                    common_names = ['pytorch_lora_weights.bin', 'lora_weights.bin', 'diffusion_pytorch_model.bin']
                    file_path = None
                    for name in common_names:
                        try:
                            file_path = hf_hub_download(repo_id=lora_path, filename=name)
                            break
                        except:
                            continue
                    
                    if not file_path:
                        raise FileNotFoundError(f"Could not find LoRA weights in {lora_path}")
                
                if file_path.endswith('.safetensors'):
                    state_dict = load_file(file_path)
                else:
                    state_dict = torch.load(file_path, map_location='cpu')
            except Exception as e:
                raise FileNotFoundError(f"Cannot find or download LoRA weights from {lora_path}: {e}")
        
        # 应用LoRA权重到DiT模型
        self._apply_lora_weights_to_dit(dit_model, state_dict, adapter_name)
        
        return state_dict

    def _apply_lora_weights_to_dit(self, dit_model, state_dict: Dict[str, torch.Tensor], adapter_name: str = "default"):
        """
        将LoRA权重应用到DiT模型
        
        Args:
            dit_model: DiT模型实例
            state_dict: LoRA权重字典
            adapter_name: 适配器名称
        """
        # 创建适配器存储（如果不存在）
        if not hasattr(self, '_lora_adapters'):
            self._lora_adapters = {}
        
        if adapter_name not in self._lora_adapters:
            self._lora_adapters[adapter_name] = {}
        
        # 遍历状态字典并应用权重
        for key, value in state_dict.items():
            # 解析键名以确定目标模块
            # 通常LoRA权重会有特定的命名约定，例如:
            # "transformer.blocks.0.attn.qkv.lora_A.weight"
            # "transformer.blocks.0.attn.qkv.lora_B.weight"
            
            try:
                # 获取目标模块
                module_keys = key.split('.')[:-1]  # 移除最后的weight/bias部分
                target_module = dit_model
                for module_key in module_keys:
                    if hasattr(target_module, module_key):
                        target_module = getattr(target_module, module_key)
                    elif module_key.isdigit() and isinstance(target_module, (list, tuple)):
                        target_module = target_module[int(module_key)]
                    else:
                        # 如果找不到模块，跳过这个权重
                        print(f"Warning: Could not find module for key {key}")
                        break
                else:
                    # 成功找到目标模块
                    param_name = key.split('.')[-1]  # 获取参数名称 (weight/bias)
                    
                    # 保存LoRA权重以便后续使用
                    self._lora_adapters[adapter_name][key] = value.clone()
                    
                    # 如果需要直接注入权重到模型中（适用于某些实现）
                    if hasattr(target_module, param_name):
                        existing_param = getattr(target_module, param_name)
                        if existing_param.shape == value.shape:
                            setattr(target_module, param_name, torch.nn.Parameter(value))
                            print(f"Injected LoRA weight for {key}")
                        else:
                            print(f"Shape mismatch for {key}: expected {existing_param.shape}, got {value.shape}")
                            
            except Exception as e:
                print(f"Warning: Failed to apply LoRA weight {key}: {e}")

    def set_lora_adapter(self, adapter_name: str = "default", weight: float = 1.0):
        """
        激活特定的LoRA适配器
        
        Args:
            adapter_name: 要激活的适配器名称
            weight: 适配器权重
        """
        if not hasattr(self, '_lora_adapters') or adapter_name not in self._lora_adapters:
            print(f"Adapter {adapter_name} not found")
            return
        
        # 在推理时应用LoRA权重调整
        # 这需要根据具体的DiT实现来定制
        print(f"Set LoRA adapter to {adapter_name} with weight {weight}")

    def unload_lora_weights(self):
        """
        卸载LoRA权重
        """
        if hasattr(self, '_lora_adapters'):
            self._lora_adapters.clear()
            print("Unloaded all LoRA weights")

    def generate(self,
                 input_prompt,
                 img,
                 img_x=None,
                 img_c=None,
                 condition=None,
                 condition_null=None,
                 max_area=720 * 1280,
                 frame_num=81,
                 shift=5.0,
                 sample_solver='unipc',
                 sampling_steps=40,
                 guide_scale=5.0,
                 condition_guide_scale=2.0,
                 n_prompt="",
                 seed=-1,
                 offload_model=True,
                 cond_input=None,
                 ):
        r"""
        Generates video frames from input image and text prompt using diffusion process.

        Args:
            input_prompt (`str`):
                Text prompt for content generation.
            img (PIL.Image.Image):
                Input image tensor. Shape: [3, H, W]
            max_area (`int`, *optional*, defaults to 720*1280):
                Maximum pixel area for latent space calculation. Controls video resolution scaling
            frame_num (`int`, *optional*, defaults to 81):
                How many frames to sample from a video. The number should be 4n+1
            shift (`float`, *optional*, defaults to 5.0):
                Noise schedule shift parameter. Affects temporal dynamics
                [NOTE]: If you want to generate a 480p video, it is recommended to set the shift value to 3.0.
            sample_solver (`str`, *optional*, defaults to 'unipc'):
                Solver used to sample the video.
            sampling_steps (`int`, *optional*, defaults to 40):
                Number of diffusion sampling steps. Higher values improve quality but slow generation
            guide_scale (`float`, *optional*, defaults 5.0):
                Text classifier-free guidance scale. Controls prompt adherence vs. creativity
            condition_guide_scale (`float`, *optional*, defaults to 2.0):
                Condition classifier-free guidance scale. Controls pose condition strength
            n_prompt (`str`, *optional*, defaults to ""):
                Negative prompt for content exclusion. If not given, use `config.sample_neg_prompt`
            seed (`int`, *optional*, defaults to -1):
                Random seed for noise generation. If -1, use random seed
            offload_model (`bool`, *optional*, defaults to True):
                If True, offloads models to CPU during generation to save VRAM

        Returns:
            torch.Tensor:
                Generated video frames tensor. Dimensions: (C, N H, W) where:
                - C: Color channels (3 for RGB)
                - N: Number of frames (81)
                - H: Frame height (from max_area)
                - W: Frame width from max_area)
        """
        if not self.wrapper:
            img = TF.to_tensor(img).sub_(0.5).div_(0.5).to(self.device)
            img_x = TF.to_tensor(img_x).sub_(0.5).div_(0.5).to(self.device)
            img_c = TF.to_tensor(img_c).sub_(0.5).div_(0.5).to(self.device)
            condition = [TF.to_tensor(c).sub_(0.5).div_(0.5).to(self.device) for c in condition]
            condition_null = [TF.to_tensor(c).sub_(0.5).div_(0.5).to(self.device) for c in condition_null]

        F = frame_num
        if not self.wrapper:
            h, w = img.shape[1:]
        else:
            h, w=img[3]*8,img[3]*8
        aspect_ratio = h / w
        lat_h = round(
            np.sqrt(max_area * aspect_ratio) // self.vae_stride[1] //
            self.patch_size[1] * self.patch_size[1])
        lat_w = round(
            np.sqrt(max_area / aspect_ratio) // self.vae_stride[2] //
            self.patch_size[2] * self.patch_size[2])
        h = lat_h * self.vae_stride[1]
        w = lat_w * self.vae_stride[2]

        max_seq_len = ((F - 1) // self.vae_stride[0] + 1) * lat_h * lat_w // (
            self.patch_size[1] * self.patch_size[2])
        max_seq_len = int(math.ceil(max_seq_len / self.sp_size)) * self.sp_size

        seed = seed if seed >= 0 else random.randint(0, sys.maxsize)
        seed_g = torch.Generator(device=self.device)
        seed_g.manual_seed(seed)
        noise = torch.randn(
            16, (F - 1) // 4 + 1,
            lat_h,
            lat_w,
            dtype=torch.float32,
            generator=seed_g,
            device=self.device)

        msk = torch.ones(1, 81, lat_h, lat_w, device=self.device)
        msk[:, 1:] = 0
        msk = torch.concat([
            torch.repeat_interleave(msk[:, 0:1], repeats=4, dim=1), msk[:, 1:]
        ],
                           dim=1)
        msk = msk.view(1, msk.shape[1] // 4, 4, lat_h, lat_w)
        msk = msk.transpose(1, 2)[0]

        if not self.wrapper:
            if n_prompt == "":
                n_prompt = self.sample_neg_prompt

            # preprocess
            if not self.t5_cpu:
                self.text_encoder.model.to(self.device)
                context = self.text_encoder([input_prompt], self.device)
                context_null = self.text_encoder([n_prompt], self.device)
                if offload_model:
                    self.text_encoder.model.cpu()
            else:
                context = self.text_encoder([input_prompt], torch.device('cpu'))
                context_null = self.text_encoder([n_prompt], torch.device('cpu'))
                context = [t.to(self.device) for t in context]
                context_null = [t.to(self.device) for t in context_null]

            self.clip.model.to(self.device)
        else:
            context=cond_input["prompt_embeds"].to(self.device)
            context_null=cond_input["negative_prompt_embeds"].to(self.device)

        # clip_context = self.clip.visual([img[:, None, :, :]])
        if not self.wrapper:
            clip_context_x = self.clip.visual([img_x[:, None, :, :]])
            clip_context_c = self.clip.visual([img_c[:, None, :, :]])
            
            if offload_model:
                self.clip.model.cpu()
        else:
            clip_context_x = cond_input["image_embeds"].to(self.device)
            clip_context_c = cond_input["image_embeds_c"].to(self.device)
        if not self.wrapper:
            y = self.vae.encode([
                torch.concat([
                    torch.nn.functional.interpolate(
                        img[None].cpu(), size=(h, w), mode='bicubic').transpose(
                            0, 1),
                    torch.zeros(3, F - 1, h, w)
                ],
                            dim=1).to(self.device)
            ])[0]
        else:
            y=img.to(msk.device)
        y = torch.concat([msk, y])
        if not self.wrapper:
            # ref img_x
            ref_x = self.vae.encode([
                torch.nn.functional.interpolate(
                    img_x[None].cpu(), size=(h, w), mode='bicubic').transpose(
                    0, 1).to(self.device)
            ])[0]
            msk_ref = torch.ones(4, 1, lat_h, lat_w, device=self.device)
            ref_x = torch.concat([ref_x, msk_ref, ref_x])

            # ref img_c
            ref_c = self.vae.encode([
                torch.nn.functional.interpolate(
                    img_c[None].cpu(), size=(h, w), mode='bicubic').transpose(
                    0, 1).to(self.device)
            ])[0]
            msk_c = torch.zeros(4, 1, lat_h, lat_w, device=self.device)
            ref_c = torch.concat([ref_c, msk_c, ref_c])

            # conditions, w/o msk
            condition = [torch.nn.functional.interpolate(
                c[None].cpu(), size=(h, w), mode='bicubic').transpose(
                0, 1) for c in condition]
            conditions = self.vae.encode([torch.cat(condition, dim=1).to(self.device)])[0]

            # conditions_null, w/o msk
            condition_null = [torch.nn.functional.interpolate(
                c[None].cpu(), size=(h, w), mode='bicubic').transpose(
                0, 1) for c in condition_null]
            conditions_null = self.vae.encode([torch.cat(condition_null, dim=1).to(self.device)])[0]
        else:
            ref_x=cond_input["ref_x"].to(self.device)
            ref_c=cond_input["ref_c"].to(self.device)
            conditions=cond_input["condition_pos"].to(self.device)
            conditions_null=cond_input["conditions_null"].to(self.device)
        @contextmanager
        def noop_no_sync():
            yield

        no_sync = getattr(self.model, 'no_sync', noop_no_sync)

        # evaluation mode
        with amp.autocast(dtype=self.param_dtype), torch.no_grad(), no_sync():

            if sample_solver == 'unipc':
                sample_scheduler = FlowUniPCMultistepScheduler(
                    num_train_timesteps=self.num_train_timesteps,
                    shift=1,
                    use_dynamic_shifting=False)
                sample_scheduler.set_timesteps(
                    sampling_steps, device=self.device, shift=shift)
                timesteps = sample_scheduler.timesteps
            elif sample_solver == 'dpm++':
                sample_scheduler = FlowDPMSolverMultistepScheduler(
                    num_train_timesteps=self.num_train_timesteps,
                    shift=1,
                    use_dynamic_shifting=False)
                sampling_sigmas = get_sampling_sigmas(sampling_steps, shift)
                timesteps, _ = retrieve_timesteps(
                    sample_scheduler,
                    device=self.device,
                    sigmas=sampling_sigmas)
            else:
                raise NotImplementedError("Unsupported solver.")

            # sample videos
            latent = noise

            arg_c = {
                'context': [context[0]],
                'clip_fea_c': clip_context_c,
                'clip_fea_x': clip_context_x,
                'seq_len': max_seq_len,
                'y': [y],
                'condition': conditions,
                'ref_c': ref_c,
                'ref_x': ref_x,
            }

            arg_null_context = {
                'context': context_null,        # null context
                'clip_fea_c': clip_context_c,
                'clip_fea_x': clip_context_x,
                'seq_len': max_seq_len,
                'y': [y],
                'condition': conditions,
                'ref_c': ref_c,
                'ref_x': ref_x,
            }

            arg_null_condition = {
                'context': [context[0]],
                'clip_fea_c': clip_context_c,
                'clip_fea_x': clip_context_x,
                'seq_len': max_seq_len,
                'y': [y],
                'condition': conditions_null,   # null condition
                'ref_c': ref_c,
                'ref_x': ref_x,
            }

            if offload_model:
                torch.cuda.empty_cache()

            self.model.to(self.device)
            for idx, t in enumerate(tqdm(timesteps)):
                latent_model_input = [latent.to(self.device)]
                timestep = [t]

                timestep = torch.stack(timestep).to(self.device)

                noise_pred_cond = self.model(
                    latent_model_input, t=timestep, **arg_c)[0].to(
                        torch.device('cpu') if offload_model else self.device)
                if offload_model:
                    torch.cuda.empty_cache()
                noise_pred_uncond_context = self.model(
                    latent_model_input, t=timestep, **arg_null_context)[0].to(
                        torch.device('cpu') if offload_model else self.device)
                if offload_model:
                    torch.cuda.empty_cache()

                if idx / len(timesteps) > self.st_cond_cfg and idx / len(timesteps) < self.end_cond_cfg:
                    noise_pred_uncond_condition = self.model(
                        latent_model_input, t=timestep, **arg_null_condition)[0].to(
                            torch.device('cpu') if offload_model else self.device)
                    if offload_model:
                        torch.cuda.empty_cache()

                    cond_context = noise_pred_cond - noise_pred_uncond_context
                    cond_condition = noise_pred_cond - noise_pred_uncond_condition                    
                    noise_pred = noise_pred_uncond_context + guide_scale * cond_context + condition_guide_scale * cond_condition
                else:
                    cond_context = noise_pred_cond - noise_pred_uncond_context
                    noise_pred = noise_pred_uncond_context + guide_scale * cond_context

                latent = latent.to(
                    torch.device('cpu') if offload_model else self.device)

                temp_x0 = sample_scheduler.step(
                    noise_pred.unsqueeze(0),
                    t,
                    latent.unsqueeze(0),
                    return_dict=False,
                    generator=seed_g)[0]
                latent = temp_x0.squeeze(0)

                x0 = [latent.to(self.device)]
                del latent_model_input, timestep

            if offload_model:
                self.model.cpu()
                torch.cuda.empty_cache()

            if self.rank == 0 and not self.wrapper:
                videos = self.vae.decode(x0)
            else:
                videos = x0
        del noise, latent
        del sample_scheduler
        if offload_model:
            gc.collect()
            torch.cuda.synchronize()
        if dist.is_initialized():
            dist.barrier()
        
        return videos[0] if  not self.wrapper else videos
